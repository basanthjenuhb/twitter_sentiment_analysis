{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos examples: (27374, 2) Neg examples: (8344, 2)\n"
     ]
    }
   ],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv(\"mydata.csv\",error_bad_lines=False, encoding = \"ISO-8859-1\",header=None)\n",
    "    data.columns = [\"SentimentText\",\"Sentiment\"]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    return data\n",
    "\n",
    "data = ingest()\n",
    "pos = data[data.Sentiment == 1]\n",
    "neg = data[data.Sentiment == 0]\n",
    "print(\"Pos examples:\",pos.shape,\"Neg examples:\", neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet).lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = [ token for token in tokens if not ( token.startswith(\"@\") or token.startswith(\"http\") or token.startswith(\"#\")) ]\n",
    "        return tokens\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 35718/35718 [00:01<00:00, 18091.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = data.shape[0]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "28574it [00:00, 443543.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "7144it [00:00, 469414.06it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_w2v = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/google.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordvector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1595it [00:00, 15943.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "3240it [00:00, 16090.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "4896it [00:00, 16228.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "6570it [00:00, 16360.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "8249it [00:00, 16486.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "9912it [00:00, 16527.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "11531it [00:00, 16424.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "13179it [00:00, 16438.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "14827it [00:00, 16449.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "16469it [00:01, 16439.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "18090it [00:01, 16369.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "19737it [00:01, 16397.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "21364it [00:01, 16357.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "22987it [00:01, 16317.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "24627it [00:01, 16341.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "26271it [00:01, 16370.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "27969it [00:01, 16548.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "28574it [00:01, 16446.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1647it [00:00, 16467.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "3223it [00:00, 16245.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "4852it [00:00, 16256.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "6459it [00:00, 16197.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "7144it [00:00, 16064.67it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np_utils.to_categorical(y_train)\n",
    "test_y = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28574 samples, validate on 7144 samples\n",
      "Epoch 1/30\n",
      "28574/28574 [==============================] - 2s - loss: 0.0248 - acc: 0.9706 - val_loss: 0.0044 - val_acc: 0.9961\n",
      "Epoch 2/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0031 - acc: 0.9970 - val_loss: 0.0033 - val_acc: 0.9968\n",
      "Epoch 3/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0024 - val_acc: 0.9978\n",
      "Epoch 4/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0023 - val_acc: 0.9978\n",
      "Epoch 5/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0023 - val_acc: 0.9978\n",
      "Epoch 6/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 7/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 8/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 9/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 10/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 11/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 12/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 13/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 14/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 15/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 16/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 17/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 18/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 19/30\n",
      "28574/28574 [==============================] - 2s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 20/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 21/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 22/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 23/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 24/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 25/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 26/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 27/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 28/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 29/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n",
      "Epoch 30/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0016 - acc: 0.9984 - val_loss: 0.0022 - val_acc: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde434f2eb8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300, kernel_initializer='normal'))\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(2, activation='softmax', kernel_initializer='normal'))\n",
    "model.compile(optimizer=Adam(lr=0.0003),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_vecs_w2v, train_y, validation_data=(test_vecs_w2v, test_y), epochs=30, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make the #InternationalYogaDay programme in #Lucknow a success, various NGOs and #Yoga gurus have camped there. https://t.co/TNuAucVF2Q 1\n",
      "[[ 0.35325179  0.64674824]]\n"
     ]
    }
   ],
   "source": [
    "m = 108\n",
    "print(data.SentimentText[m], data.Sentiment[m])\n",
    "scentence = data.tokens[m]\n",
    "scentence = tokenize(\"Kovind will make an exceptional president: PM Narendra Modi\")\n",
    "vec = wordvector(scentence, 300)\n",
    "print(model.predict(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
