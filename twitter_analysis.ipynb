{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos examples: (27605, 2) Neg examples: (8395, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Mayank_M_Joshi: Kovind will make an except...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@narendramodi @ashrafghani Hii pm modi good mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this \" love for Dalit \" emerged to sidelin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @aijazzakasyed: @AijazZakaSyed writes in @S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@soniandtv @vivekagnihotri MamatajiÃ°ÂŸÂ˜Â‚Ã°ÂŸÂ˜Â‚Ã°ÂŸÂ˜...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  RT @Mayank_M_Joshi: Kovind will make an except...          1\n",
       "1  @narendramodi @ashrafghani Hii pm modi good mo...          1\n",
       "2  And this \" love for Dalit \" emerged to sidelin...          1\n",
       "3  RT @aijazzakasyed: @AijazZakaSyed writes in @S...          1\n",
       "4  @soniandtv @vivekagnihotri MamatajiÃ°ÂŸÂ˜Â‚Ã°ÂŸÂ˜Â‚Ã°ÂŸÂ˜...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "# Convert the labels from str to int.\n",
    "def getData():\n",
    "    data = pd.read_csv(\"dataset/mydata.csv\",error_bad_lines=False, encoding = \"ISO-8859-1\",header=None)\n",
    "    data.columns = [\"SentimentText\",\"Sentiment\"]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    return data\n",
    "\n",
    "data = getData()\n",
    "pos = data[data.Sentiment == 1]\n",
    "neg = data[data.Sentiment == 0]\n",
    "print(\"Pos examples:\",pos.shape,\"Neg examples:\", neg.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "# Remove the tokens which start with '@' or 'http' or '#'\n",
    "# Replace slangs in the tweets. Ex: k -> ok, u -> you\n",
    "# Replace emojis with corresponging text. Ex: ðŸ˜‚ -> face with tears of joy\n",
    "slangs = pickle.load(open(\"dataset/slang.pkl\",\"rb\"))\n",
    "slangs['&'] = 'and'\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet).lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = [ token for token in tokens if not ( token.startswith(\"@\") or token.startswith(\"http\") or token.startswith(\"#\")) ]\n",
    "        final_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            try:\n",
    "                tokens[i] = slangs[tokens[i]]\n",
    "            except:\n",
    "                continue\n",
    "        for i in range(len(tokens)):\n",
    "            try:\n",
    "                words = emoji.UNICODE_EMOJI[tokens[i]][1:-1].split(\"_\")\n",
    "                final_tokens += words\n",
    "            except:\n",
    "                final_tokens.append(tokens[i])\n",
    "        return final_tokens\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36000/36000 [00:02<00:00, 14340.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break the data into test train split\n",
    "n = data.shape[0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28800it [00:00, 469126.90it/s]\n",
      "7200it [00:00, 477280.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the google's trained word2vec model\n",
    "tweet_w2v = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/google.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting a scentence to vector\n",
    "# Basically averaging the vectors of different words\n",
    "def wordvector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28800it [00:01, 16449.52it/s]\n",
      "7200it [00:00, 16382.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting labels to one hot vector\n",
    "# 0 -> [0,1]\n",
    "# 1 - [1,0]\n",
    "train_y = np_utils.to_categorical(y_train)\n",
    "test_y = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28800 samples, validate on 7200 samples\n",
      "Epoch 1/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.1572 - acc: 0.9502 - val_loss: 0.0311 - val_acc: 0.9969\n",
      "Epoch 2/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0196 - acc: 0.9965 - val_loss: 0.0079 - val_acc: 0.9986\n",
      "Epoch 3/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0068 - acc: 0.9990 - val_loss: 0.0054 - val_acc: 0.9989\n",
      "Epoch 4/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0037 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9999\n",
      "Epoch 5/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0031 - val_acc: 0.9989\n",
      "Epoch 6/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0020 - acc: 0.9994 - val_loss: 7.5142e-04 - val_acc: 0.9999\n",
      "Epoch 7/30\n",
      "28800/28800 [==============================] - 1s - loss: 7.1750e-04 - acc: 0.9999 - val_loss: 9.5210e-04 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0014 - acc: 0.9995 - val_loss: 6.5156e-04 - val_acc: 0.9999\n",
      "Epoch 9/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0015 - acc: 0.9997 - val_loss: 4.7832e-04 - val_acc: 0.9999\n",
      "Epoch 10/30\n",
      "28800/28800 [==============================] - 1s - loss: 8.5717e-04 - acc: 0.9997 - val_loss: 0.0043 - val_acc: 0.9989\n",
      "Epoch 11/30\n",
      "28800/28800 [==============================] - 1s - loss: 0.0019 - acc: 0.9996 - val_loss: 3.4148e-04 - val_acc: 0.9999\n",
      "Epoch 12/30\n",
      "28800/28800 [==============================] - 1s - loss: 5.1184e-04 - acc: 0.9999 - val_loss: 3.2408e-04 - val_acc: 0.9999\n",
      "Epoch 13/30\n",
      "28800/28800 [==============================] - 1s - loss: 4.8455e-04 - acc: 0.9999 - val_loss: 3.4860e-04 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "28800/28800 [==============================] - 1s - loss: 2.1493e-04 - acc: 1.0000 - val_loss: 1.4990e-04 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "28800/28800 [==============================] - 1s - loss: 1.4706e-04 - acc: 1.0000 - val_loss: 1.6638e-04 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "28800/28800 [==============================] - 1s - loss: 6.6086e-05 - acc: 1.0000 - val_loss: 1.1269e-04 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "28800/28800 [==============================] - 1s - loss: 5.9501e-05 - acc: 1.0000 - val_loss: 1.1198e-04 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "28800/28800 [==============================] - 1s - loss: 3.9166e-05 - acc: 1.0000 - val_loss: 9.1433e-05 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "28800/28800 [==============================] - 1s - loss: 3.2139e-05 - acc: 1.0000 - val_loss: 8.3973e-05 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "28800/28800 [==============================] - 1s - loss: 2.1655e-05 - acc: 1.0000 - val_loss: 9.9057e-05 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "28800/28800 [==============================] - 1s - loss: 1.0360e-04 - acc: 1.0000 - val_loss: 9.5571e-05 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "28800/28800 [==============================] - 1s - loss: 5.8249e-04 - acc: 0.9999 - val_loss: 9.7058e-05 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "28800/28800 [==============================] - 1s - loss: 2.1076e-05 - acc: 1.0000 - val_loss: 7.9430e-05 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "28800/28800 [==============================] - 1s - loss: 1.4024e-05 - acc: 1.0000 - val_loss: 7.4902e-05 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "28800/28800 [==============================] - 1s - loss: 1.1674e-05 - acc: 1.0000 - val_loss: 7.0061e-05 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "28800/28800 [==============================] - 1s - loss: 8.6723e-06 - acc: 1.0000 - val_loss: 6.6022e-05 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "28800/28800 [==============================] - 1s - loss: 7.2670e-06 - acc: 1.0000 - val_loss: 6.4478e-05 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "28800/28800 [==============================] - 1s - loss: 6.3453e-06 - acc: 1.0000 - val_loss: 6.1642e-05 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "28800/28800 [==============================] - 1s - loss: 5.0760e-06 - acc: 1.0000 - val_loss: 5.9929e-05 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "28800/28800 [==============================] - 1s - loss: 4.0938e-06 - acc: 1.0000 - val_loss: 5.8588e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44c6d6b7b8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with 2 hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300, kernel_initializer='normal'))\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(2, activation='softmax', kernel_initializer='normal'))\n",
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_vecs_w2v, train_y, validation_data=(test_vecs_w2v, test_y), epochs=30, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['modi', 'govt', 'is', 'doing', 'a', 'great', 'job', 'face', 'with', 'tears', 'of', 'joy', 'face', 'with', 'tears', 'of', 'joy', 'face', 'with', 'tears', 'of', 'joy']\n",
      "[[ 0.1157992   0.88420075]]\n"
     ]
    }
   ],
   "source": [
    "scentence = tokenize(\"Modi govt is doing a great job ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚\")\n",
    "print(scentence)\n",
    "vec = wordvector(scentence, 300)\n",
    "print(model.predict(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
