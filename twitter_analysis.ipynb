{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos examples: (27374, 2) Neg examples: (8344, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Mayank_M_Joshi: Kovind will make an except...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@narendramodi @ashrafghani Hii pm modi good mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this \" love for Dalit \" emerged to sidelin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @aijazzakasyed: @AijazZakaSyed writes in @S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@soniandtv @vivekagnihotri Mamatajiððð...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  RT @Mayank_M_Joshi: Kovind will make an except...          1\n",
       "1  @narendramodi @ashrafghani Hii pm modi good mo...          1\n",
       "2  And this \" love for Dalit \" emerged to sidelin...          1\n",
       "3  RT @aijazzakasyed: @AijazZakaSyed writes in @S...          1\n",
       "4  @soniandtv @vivekagnihotri Mamatajiððð...          1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "# Convert the labels from str to int.\n",
    "def getData():\n",
    "    data = pd.read_csv(\"mydata.csv\",error_bad_lines=False, encoding = \"ISO-8859-1\",header=None)\n",
    "    data.columns = [\"SentimentText\",\"Sentiment\"]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    return data\n",
    "\n",
    "data = getData()\n",
    "pos = data[data.Sentiment == 1]\n",
    "neg = data[data.Sentiment == 0]\n",
    "print(\"Pos examples:\",pos.shape,\"Neg examples:\", neg.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "# Remove the tokens which start with '@' or 'http' or '#\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet).lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = [ token for token in tokens if not ( token.startswith(\"@\") or token.startswith(\"http\") or token.startswith(\"#\")) ]\n",
    "        return tokens\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "progress-bar:   0%|          | 0/35718 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:   5%|▍         | 1725/35718 [00:00<00:01, 17241.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  10%|▉         | 3441/35718 [00:00<00:01, 17215.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  15%|█▍        | 5225/35718 [00:00<00:01, 17397.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  19%|█▉        | 6925/35718 [00:00<00:01, 17274.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  24%|██▍       | 8703/35718 [00:00<00:01, 17422.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  30%|██▉       | 10664/35718 [00:00<00:01, 18024.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  35%|███▍      | 12422/35718 [00:00<00:01, 17887.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  40%|████      | 14363/35718 [00:00<00:01, 18316.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  45%|████▌     | 16101/35718 [00:00<00:01, 17966.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  50%|████▉     | 17834/35718 [00:01<00:01, 17671.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  55%|█████▍    | 19558/35718 [00:01<00:00, 17245.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  60%|██████    | 21451/35718 [00:01<00:00, 17718.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  65%|██████▍   | 23207/35718 [00:01<00:00, 17634.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  70%|██████▉   | 24959/35718 [00:01<00:00, 17549.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  75%|███████▌  | 26818/35718 [00:01<00:00, 17848.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  80%|████████  | 28637/35718 [00:01<00:00, 17948.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  85%|████████▌ | 30437/35718 [00:01<00:00, 17962.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  90%|█████████ | 32232/35718 [00:01<00:00, 17594.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar:  95%|█████████▌| 34057/35718 [00:01<00:00, 17784.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "progress-bar: 100%|██████████| 35718/35718 [00:02<00:00, 17814.68it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break the data into test train split\n",
    "n = data.shape[0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "28574it [00:00, 437981.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "7144it [00:00, 466919.75it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the google's trained word3vec model\n",
    "tweet_w2v = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/google.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting a scentence to vector\n",
    "# Basically averaging the vectors of different words\n",
    "def wordvector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1792it [00:00, 17916.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "3550it [00:00, 17813.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "5335it [00:00, 17822.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "7077it [00:00, 17697.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "8892it [00:00, 17830.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "10644it [00:00, 17734.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "12379it [00:00, 17613.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "14144it [00:00, 17623.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "15913it [00:00, 17643.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "17651it [00:01, 17562.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "19391it [00:01, 17510.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "21149it [00:01, 17522.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "22911it [00:01, 17549.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "24683it [00:01, 17599.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "26432it [00:01, 17564.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "28181it [00:01, 17513.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "28574it [00:01, 17601.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1749it [00:00, 17487.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "3499it [00:00, 17490.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "5253it [00:00, 17502.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "6943it [00:00, 17316.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "7144it [00:00, 17334.22it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting labels to one hot vector\n",
    "# 0 -> [0,1]\n",
    "# 1 - [1,0]\n",
    "train_y = np_utils.to_categorical(y_train)\n",
    "test_y = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28574 samples, validate on 7144 samples\n",
      "Epoch 1/30\n",
      "28574/28574 [==============================] - 2s - loss: 0.0372 - acc: 0.9617 - val_loss: 0.0071 - val_acc: 0.9957\n",
      "Epoch 2/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0046 - acc: 0.9966 - val_loss: 0.0022 - val_acc: 0.9989\n",
      "Epoch 3/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0020 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 0.9989\n",
      "Epoch 4/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0020 - acc: 0.9981 - val_loss: 0.0013 - val_acc: 0.9989\n",
      "Epoch 5/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0014 - acc: 0.9986 - val_loss: 2.7358e-04 - val_acc: 0.9999\n",
      "Epoch 6/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.3568e-04 - acc: 0.9995 - val_loss: 2.0094e-04 - val_acc: 0.9999\n",
      "Epoch 7/30\n",
      "28574/28574 [==============================] - 1s - loss: 8.6296e-05 - acc: 1.0000 - val_loss: 1.7644e-04 - val_acc: 0.9999\n",
      "Epoch 8/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.5518e-05 - acc: 1.0000 - val_loss: 1.6501e-04 - val_acc: 0.9999\n",
      "Epoch 9/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.9657e-05 - acc: 1.0000 - val_loss: 1.4765e-04 - val_acc: 0.9999\n",
      "Epoch 10/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.8664e-05 - acc: 1.0000 - val_loss: 1.4241e-04 - val_acc: 0.9999\n",
      "Epoch 11/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.1265e-05 - acc: 1.0000 - val_loss: 1.3844e-04 - val_acc: 0.9999\n",
      "Epoch 12/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.6081e-05 - acc: 1.0000 - val_loss: 1.3265e-04 - val_acc: 0.9999\n",
      "Epoch 13/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.2705e-05 - acc: 1.0000 - val_loss: 1.3178e-04 - val_acc: 0.9999\n",
      "Epoch 14/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.0065e-05 - acc: 1.0000 - val_loss: 1.2997e-04 - val_acc: 0.9999\n",
      "Epoch 15/30\n",
      "28574/28574 [==============================] - 1s - loss: 7.9491e-06 - acc: 1.0000 - val_loss: 1.3013e-04 - val_acc: 0.9999\n",
      "Epoch 16/30\n",
      "28574/28574 [==============================] - 1s - loss: 6.4304e-06 - acc: 1.0000 - val_loss: 1.2981e-04 - val_acc: 0.9999\n",
      "Epoch 17/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.1370e-06 - acc: 1.0000 - val_loss: 1.2949e-04 - val_acc: 0.9999\n",
      "Epoch 18/30\n",
      "28574/28574 [==============================] - 1s - loss: 4.2420e-06 - acc: 1.0000 - val_loss: 1.2992e-04 - val_acc: 0.9999\n",
      "Epoch 19/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.4870e-06 - acc: 1.0000 - val_loss: 1.2990e-04 - val_acc: 0.9999\n",
      "Epoch 20/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.8614e-06 - acc: 1.0000 - val_loss: 1.2984e-04 - val_acc: 0.9999\n",
      "Epoch 21/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.3560e-06 - acc: 1.0000 - val_loss: 1.3035e-04 - val_acc: 0.9999\n",
      "Epoch 22/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.9507e-06 - acc: 1.0000 - val_loss: 1.3053e-04 - val_acc: 0.9999\n",
      "Epoch 23/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.6251e-06 - acc: 1.0000 - val_loss: 1.3069e-04 - val_acc: 0.9999\n",
      "Epoch 24/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.3484e-06 - acc: 1.0000 - val_loss: 1.3086e-04 - val_acc: 0.9999\n",
      "Epoch 25/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.1276e-06 - acc: 1.0000 - val_loss: 1.3091e-04 - val_acc: 0.9999\n",
      "Epoch 26/30\n",
      "28574/28574 [==============================] - 1s - loss: 9.2878e-07 - acc: 1.0000 - val_loss: 1.3144e-04 - val_acc: 0.9999\n",
      "Epoch 27/30\n",
      "28574/28574 [==============================] - 1s - loss: 7.8722e-07 - acc: 1.0000 - val_loss: 1.3208e-04 - val_acc: 0.9999\n",
      "Epoch 28/30\n",
      "28574/28574 [==============================] - 2s - loss: 6.5288e-07 - acc: 1.0000 - val_loss: 1.3182e-04 - val_acc: 0.9999\n",
      "Epoch 29/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.4647e-07 - acc: 1.0000 - val_loss: 1.3238e-04 - val_acc: 0.9999\n",
      "Epoch 30/30\n",
      "28574/28574 [==============================] - 1s - loss: 4.6148e-07 - acc: 1.0000 - val_loss: 1.3264e-04 - val_acc: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde0a69d358>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with 2 hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300, kernel_initializer='normal'))\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(2, activation='softmax', kernel_initializer='normal'))\n",
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_vecs_w2v, train_y, validation_data=(test_vecs_w2v, test_y), epochs=30, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make the #InternationalYogaDay programme in #Lucknow a success, various NGOs and #Yoga gurus have camped there. https://t.co/TNuAucVF2Q 1\n",
      "[[ 0.30909264  0.69090742]]\n"
     ]
    }
   ],
   "source": [
    "m = 108\n",
    "print(data.SentimentText[m], data.Sentiment[m])\n",
    "scentence = data.tokens[m]\n",
    "scentence = tokenize(\"Kovind will make an exceptional president: PM Narendra Modi\")\n",
    "vec = wordvector(scentence, 300)\n",
    "print(model.predict(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
