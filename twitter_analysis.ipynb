{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos examples: (27374, 2) Neg examples: (8344, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Mayank_M_Joshi: Kovind will make an except...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@narendramodi @ashrafghani Hii pm modi good mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this \" love for Dalit \" emerged to sidelin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @aijazzakasyed: @AijazZakaSyed writes in @S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@soniandtv @vivekagnihotri Mamatajiððð...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  RT @Mayank_M_Joshi: Kovind will make an except...          1\n",
       "1  @narendramodi @ashrafghani Hii pm modi good mo...          1\n",
       "2  And this \" love for Dalit \" emerged to sidelin...          1\n",
       "3  RT @aijazzakasyed: @AijazZakaSyed writes in @S...          1\n",
       "4  @soniandtv @vivekagnihotri Mamatajiððð...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "# Convert the labels from str to int.\n",
    "def getData():\n",
    "    data = pd.read_csv(\"dataset/mydata.csv\",error_bad_lines=False, encoding = \"ISO-8859-1\",header=None)\n",
    "    data.columns = [\"SentimentText\",\"Sentiment\"]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    return data\n",
    "\n",
    "data = getData()\n",
    "pos = data[data.Sentiment == 1]\n",
    "neg = data[data.Sentiment == 0]\n",
    "print(\"Pos examples:\",pos.shape,\"Neg examples:\", neg.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "# Remove the tokens which start with '@' or 'http' or '#\n",
    "slangs = pickle.load(open(\"dataset/slang.pkl\",\"rb\"))\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet).lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = [ token for token in tokens if not ( token.startswith(\"@\") or token.startswith(\"http\") or token.startswith(\"#\")) ]\n",
    "        for i in range(len(tokens)):\n",
    "            try:\n",
    "                tokens[i] = slangs[tokens[i]]\n",
    "            except:\n",
    "                continue\n",
    "        return tokens\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 35718/35718 [00:02<00:00, 16310.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break the data into test train split\n",
    "n = data.shape[0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28574it [00:00, 501962.41it/s]\n",
      "7144it [00:00, 494702.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the google's trained word2vec model\n",
    "tweet_w2v = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/google.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting a scentence to vector\n",
    "# Basically averaging the vectors of different words\n",
    "def wordvector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28574it [00:01, 17657.75it/s]\n",
      "7144it [00:00, 18503.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([wordvector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting labels to one hot vector\n",
    "# 0 -> [0,1]\n",
    "# 1 - [1,0]\n",
    "train_y = np_utils.to_categorical(y_train)\n",
    "test_y = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28574 samples, validate on 7144 samples\n",
      "Epoch 1/30\n",
      "28574/28574 [==============================] - 2s - loss: 0.0484 - acc: 0.9488 - val_loss: 0.0118 - val_acc: 0.9894\n",
      "Epoch 2/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0066 - acc: 0.9950 - val_loss: 0.0056 - val_acc: 0.9943\n",
      "Epoch 3/30\n",
      "28574/28574 [==============================] - 2s - loss: 0.0029 - acc: 0.9975 - val_loss: 0.0013 - val_acc: 0.9987\n",
      "Epoch 4/30\n",
      "28574/28574 [==============================] - 1s - loss: 0.0013 - acc: 0.9990 - val_loss: 6.1183e-04 - val_acc: 0.9993\n",
      "Epoch 5/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.4864e-04 - acc: 0.9997 - val_loss: 4.2846e-04 - val_acc: 0.9996\n",
      "Epoch 6/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.0154e-04 - acc: 0.9999 - val_loss: 3.8725e-04 - val_acc: 0.9996\n",
      "Epoch 7/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.3587e-04 - acc: 0.9999 - val_loss: 3.3796e-04 - val_acc: 0.9996\n",
      "Epoch 8/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.0230e-04 - acc: 0.9999 - val_loss: 3.1738e-04 - val_acc: 0.9996\n",
      "Epoch 9/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.8029e-04 - acc: 0.9999 - val_loss: 3.3289e-04 - val_acc: 0.9996\n",
      "Epoch 10/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.1719e-04 - acc: 0.9996 - val_loss: 3.0713e-04 - val_acc: 0.9996\n",
      "Epoch 11/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.5427e-05 - acc: 1.0000 - val_loss: 2.8167e-04 - val_acc: 0.9996\n",
      "Epoch 12/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.7880e-05 - acc: 1.0000 - val_loss: 2.7452e-04 - val_acc: 0.9996\n",
      "Epoch 13/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.3893e-05 - acc: 1.0000 - val_loss: 2.7662e-04 - val_acc: 0.9996\n",
      "Epoch 14/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.0898e-05 - acc: 1.0000 - val_loss: 2.8700e-04 - val_acc: 0.9996\n",
      "Epoch 15/30\n",
      "28574/28574 [==============================] - 1s - loss: 8.7085e-06 - acc: 1.0000 - val_loss: 2.9293e-04 - val_acc: 0.9996\n",
      "Epoch 16/30\n",
      "28574/28574 [==============================] - 1s - loss: 7.1355e-06 - acc: 1.0000 - val_loss: 2.8822e-04 - val_acc: 0.9996\n",
      "Epoch 17/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.7671e-06 - acc: 1.0000 - val_loss: 2.8125e-04 - val_acc: 0.9996\n",
      "Epoch 18/30\n",
      "28574/28574 [==============================] - 1s - loss: 4.7254e-06 - acc: 1.0000 - val_loss: 2.9249e-04 - val_acc: 0.9996\n",
      "Epoch 19/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.8894e-06 - acc: 1.0000 - val_loss: 2.8960e-04 - val_acc: 0.9996\n",
      "Epoch 20/30\n",
      "28574/28574 [==============================] - 1s - loss: 3.1888e-06 - acc: 1.0000 - val_loss: 2.9542e-04 - val_acc: 0.9996\n",
      "Epoch 21/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.6075e-06 - acc: 1.0000 - val_loss: 2.9590e-04 - val_acc: 0.9996\n",
      "Epoch 22/30\n",
      "28574/28574 [==============================] - 1s - loss: 2.1634e-06 - acc: 1.0000 - val_loss: 2.9824e-04 - val_acc: 0.9996\n",
      "Epoch 23/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.7636e-06 - acc: 1.0000 - val_loss: 3.0590e-04 - val_acc: 0.9996\n",
      "Epoch 24/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.4775e-06 - acc: 1.0000 - val_loss: 2.9855e-04 - val_acc: 0.9996\n",
      "Epoch 25/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.2533e-06 - acc: 1.0000 - val_loss: 3.0649e-04 - val_acc: 0.9996\n",
      "Epoch 26/30\n",
      "28574/28574 [==============================] - 1s - loss: 1.0132e-06 - acc: 1.0000 - val_loss: 3.0587e-04 - val_acc: 0.9996\n",
      "Epoch 27/30\n",
      "28574/28574 [==============================] - 1s - loss: 8.4122e-07 - acc: 1.0000 - val_loss: 3.0686e-04 - val_acc: 0.9996\n",
      "Epoch 28/30\n",
      "28574/28574 [==============================] - 1s - loss: 7.0233e-07 - acc: 1.0000 - val_loss: 3.0600e-04 - val_acc: 0.9996\n",
      "Epoch 29/30\n",
      "28574/28574 [==============================] - 1s - loss: 5.8643e-07 - acc: 1.0000 - val_loss: 3.1013e-04 - val_acc: 0.9996\n",
      "Epoch 30/30\n",
      "28574/28574 [==============================] - 1s - loss: 4.8739e-07 - acc: 1.0000 - val_loss: 3.1115e-04 - val_acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa26eb51cc0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a model with 2 hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300, kernel_initializer='normal'))\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='normal'))\n",
    "model.add(Dense(2, activation='softmax', kernel_initializer='normal'))\n",
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_vecs_w2v, train_y, validation_data=(test_vecs_w2v, test_y), epochs=30, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make the #InternationalYogaDay programme in #Lucknow a success, various NGOs and #Yoga gurus have camped there. https://t.co/TNuAucVF2Q 1\n",
      "[[ 0.31715828  0.68284172]]\n"
     ]
    }
   ],
   "source": [
    "m = 108\n",
    "print(data.SentimentText[m], data.Sentiment[m])\n",
    "scentence = data.tokens[m]\n",
    "scentence = tokenize(\"Kovind will make an exceptional president: PM Narendra Modi\")\n",
    "vec = wordvector(scentence, 300)\n",
    "print(model.predict(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
